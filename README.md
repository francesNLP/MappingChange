# ğŸ—ºï¸ MappingChange

## Tracking the Evolution of Place Descriptions in the Gazetteers of Scotland (1803â€“1901)
This repository supports a research project to transform [The Gazetteers of Scotland (1803â€“1901)](https://data.nls.uk/data/digitised-collections/gazetteers-of-scotland/), digitized by the National Library of Scotland (NLS), into structured article-level data. These gazetteers provide detailed historical accounts of Scottish placesâ€”towns, glens, castles, and parishesâ€”captured across 19 volumes (10 editions):
  
<img src="./Notebooks/figures/gazetteers_vols.png" alt="Number of vol per edition" width="700"/>

The goal is to extract these entries from OCR-based page-level text and convert them into cleaned, deduplicated article records that can eventually populate a temporal and semantic knowledge graph (ScotGaz19-KG). This graph will be integrated into the [Frances platform](http://www.frances-ai.com), enabling rich visualizations and advanced NLP-driven analysis of Scotlandâ€™s historical landscape.

This work forms part of the RSE-funded project and builds on prior research funded by the National Library of Scotland.

## ğŸ“° Associated Publication

This repository supports the ISWC 2025 Resources Track paper:

**Mapping Change: A Temporal Knowledge Graph of Scottish Gazetteers (1803â€“1901)**  
- Authors: Lilin Yu, Rosa Filgueira 
- Submitted to: *ISWC 2025 â€“ Resources Track* 
- ğŸ“¦ [Zenodo archive (DOI)](https://doi.org/10.5281/zenodo.XXXXXXX)  
- All code and data available at [github.com/francesNLP/MappingChange](https://github.com/francesNLP/MappingChange)

## âš™ï¸ Setup Instructions
```bash
conda create -n gazetteer_env python=3.11 -y
conda activate gazetteer_env
pip install -r requirements.txt
```

Required:

- OpenAI API key
- Fuseki + Elasticsearch server credentials
- [Base dataframe at page-level: gazetteers_dataframe](./https://drive.google.com/file/d/1J6TxdKImw2rNgmdUBN19h202gl-iYupn/view?usp=share_link)
- Countries KG
- Edinburgh Geoparser

## ğŸ“š Overview and Pipeline

This repository provides a complete pipeline for transforming the [Gazetteers of Scotland (1803â€“1901)](https://data.nls.uk/data/digitised-collections/gazetteers-of-scotland/) into structured, semantically enriched article-level data. These Gazetteers are digitized as page-by-page OCR text and span 19 volumes across 10 historical editions.

Our goal is to extract clean, deduplicated records of place descriptions and represent them in a temporal knowledge graph aligned with the [Heritage Textual Ontology (HTO)](https://w3id.org/hto). The outputs enable downstream analysis of cultural, geographical, and editorial change across 19th-century Scotland.

We use the preprocessed dataframe [`gazetteers_dataframe`](https://zenodo.org/records/14051678), which contains OCR and metadata at the page level, as the main input to the pipeline.

<div align="center">
  <img src="pipeline_overview.png" alt="Pipeline Overview" width="600"/>
</div>

### ğŸ”„ Pipeline Stages

- **1. Article Extraction**: Segment OCR pages into article-level entries using GPT-4, customized per edition.
- **2. Cleaning & Deduplication**: Merge partial results, resolve duplicates and enrich with original XML metadata.
- **3. Knowledge Graph Construction**: Convert articles into RDF triples using HTO, with links between redirected and referenced entries.
- **4. Semantic Enrichment**:
  - Link articles across editions into temporal concepts using embeddings.
  - Align with external sources (Wikidata, DBpedia).
  - Annotate locations via NER and georesolution (Stanza + Edinburgh Geoparser).
- **5. Indexing**: Export enriched data into Elasticsearch for semantic and full-text search.

Each of these stages is modular and well-documented in the `src/` directory, with configuration examples and output formats clearly specified in the [Execution Walkthrough](#ğŸš€-pipeline-execution-walkthrough).

## ğŸš€ Pipeline Execution Guide

For full setup instructions and detailed script walkthroughs, see the dedicated guide:

ğŸ‘‰ [ğŸ“„ PIPELINE_EXECUTION.md](./PIPELINE_EXECUTION.md)

This includes:
- Environment setup
- GPT-based extraction
- Data cleaning & merging
- Knowledge graph generation
- Semantic and spatial enrichment
- SPARQL validation queries
- Elasticsearch indexing


## Dataframes with Extracted Articles

These cleaned, deduplicated DataFrames (as a result of running[dataframe_articles.py](./src/dataframe_articles.py) wich each dataset) are ready for semantic enrichment and visual analysis:

* [dataframe_gaz_1803](https://drive.google.com/file/d/1a4BtLrwyfHb4I6cmAVbaaw-IafWf1dnR/view?usp=share_link)
* [dataframe_gaz_1806](https://drive.google.com/file/d/1ZGt8hKzQ2rvk_-dlVHpn6UwoSkiZyNDO/view?usp=share_link)
* [dataframe_gaz_1825](https://drive.google.com/file/d/1Fsr61JqpV4JND0VKtezbNoVCrdw_Ahi4/view?usp=share_link)
* [dataframe_gaz_1838](https://drive.google.com/file/d/1g5xCuG_eAJp0GQNfDDTpwSK4ndqTz-G_/view?usp=share_link)
* [dataframe_gaz_1842](https://drive.google.com/file/d/1dNJaS9RWHOvP3vsfy5ZDE6SCiVeSiRj_/view?usp=share_link)
* [dataframe_gaz_1846](https://drive.google.com/file/d/1JxGybA-op04Xvs6-MG-C6x1iuneLF5qQ/view?usp=share_link)
* [dataframe_gaz_1868](https://drive.google.com/file/d/1thPWG2LXHvo7owEWOzu_K_B5XZ5znPMO/view?usp=share_link)
* [dataframe_gaz_1882](https://drive.google.com/file/d/1r5DMWfOas_ajS71vrC0Cr4I3oxD6ZLjm/view?usp=share_link)
* [dataframe_gaz_1884](https://drive.google.com/file/d/1EHrlwH5cnZb1QISt_98ZcEpIVP3wIHmt/view?usp=share_link)
* [dataframe_gaz_1901](https://drive.google.com/file/d/1a3Qi0Oj8HzFql0BkPjutaUQx8fSzqy1C/view?usp=share_link)


**Important**: The aggreated dataframe, which also includes embeddings, can be downloaded from here: [`gaz_kg_concepts_df`](https://drive.google.com/file/d/1EyG_Jm5so6bGL6is9Br8eDs5gVutKdQX/view?usp=share_link)



## ğŸ““ Notebooks Exploration

These Jupyter notebooks offer different entry points for exploring the Gazetteers of Scotland dataset, enriched by the MappingChange pipeline:

- [`Exploring_Individual_Gz_Dataframes.ipynb`](./Notebooks/Exploring_Individual_Gz_Dataframes.ipynb): Explores each gazetteer edition separately â€” useful for comparing formatting, cleaning strategies, and early article-level insights.

- [`Exploring_AggregatedDF.ipynb`](./Notebooks/Exploring_AggregatedDF.ipynb): Main exploratory notebook working with the unified DataFrame (`gaz_kg_concepts_df`). Includes 22 analyses covering article counts, sentiment, keyword trends, embeddings, and semantic change across editions.

- [`Knowledge_Exploration_SPARQL.ipynb`](./Notebooks/Knowledge_Exploration_SPARQL.ipynb):Queries the Gazetteers Knowledge Graph using SPARQL. Enables structured exploration of linked data, references, and ontology-backed relations.

Each notebook serves a different aspect of the project: data quality, temporal-linguistic analysis, and semantic web querying.


### ğŸ“Š Comparative Analyses

The notebook [`Exploring_AggregatedDF.ipynb`](./Notebooks/Exploring_AggregatedDF.ipynb) provides a comprehensive suite of 22 comparative analyses across editions of the Gazetteers. These include trends in article length, sentiment, keyword frequency, semantic similarity, and named entity evolutionâ€”offering insights into how place descriptions shift across time.

Below, we showcase just two representative visualizations:

#### 1. Keyword Trends Over Time
A heatmap showing how terms related to religion, gender, industry, education, and governance vary across editions (1803â€“1901).

<img src="./Notebooks/figures/keyword_heatmap_small.png" alt="Keyword Heatmap" width="700"/>

#### 2. Article Length Distribution
A boxplot of word counts per article by edition, highlighting editorial and structural variation across volumes.

<img src="./Notebooks/figures/wordcount_boxplot_small.png" alt="Word Count Boxplot" width="700"/>

These examples illustrate the types of diachronic comparisons made possible by the MappingChange pipeline. For a deeper dive into all analyses, see the full [`Exploring_AggregatedDF.ipynb`](./Notebooks/Exploring_AggregatedDF.ipynb).

The full notebook includes the following types of analyses:

- ğŸ“Š Article count and length per edition
- ğŸ“¦ Boxplot of article word counts across years
- ğŸ“ˆ Top longest article by edition
- ğŸ”¤ First-letter distribution of article titles
- ğŸ§  Most frequent adjectives per edition (using SpaCy)
- ğŸ·ï¸ Most referenced capitalized phrases (place-like terms)
- ğŸ” Repeated place names within each edition
- ğŸ”„ Place names reused across multiple editions
- ğŸ§® Alternate name statistics (redirects and variants)
- ğŸ”— Reference term usage and density per edition
- ğŸ“‰ Keyword frequency trends for selected terms (e.g. railway, harbour)
- ğŸ“Œ Keyword frequency heatmap across ~40 terms (e.g. church, cotton, parliament)
- ğŸ§  TF-IDF keyword analysis (top distinctive terms per edition)
- ğŸ” TF-IDF trend tracking of specific words (e.g. "railway", "church")
- ğŸ’¬ Sentiment analysis of article texts across editions (VADER)
- ğŸ™ï¸ Sentiment over time for selected places (e.g. Edinburgh)
- ğŸŒ Geocoding sample articles using Nominatim + Folium
- ğŸ“ Semantic similarity of articles (using precomputed embeddings)
- ğŸ§­ Ranking places by semantic drift over time
- ğŸ§¾ Comparison of article lengths (pages, words) for specific places
- ğŸ“ Historical narrative shifts in key cities (e.g. Glasgow, Edinburgh)
- ğŸ“š Side-by-side text comparisons of places across editions

These analyses help uncover editorial, linguistic, and conceptual changes in how Scottish places were described from 1803 to 1901.



## âœ¨ Research Context

This work contributes to the [MappingChange initiative](https://rse.org.uk/scotlands-vibrant-research-sector-to-receive-over-705-5k-in-the-latest-rse-research-awards-programme/): building a temporal and semantic knowledge graph of 19th-century Scottish place descriptions. It enables researchers to:

- Analyze the evolution of geographical and cultural narratives
- Compare local descriptions in the Gazetteers with national perspectives in the Encyclopaedia Britannica
- Link and cluster places across editions and sources using NLP and semantic matching
- The extracted articles will be integrated into Frances, an AI-driven platform for historical text analysis hosted at the Edinburgh International Data Facility (EIDF).
